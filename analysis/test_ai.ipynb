{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m## IMPORTS\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader, Dataset\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertTokenizer, BertForTokenClassification, AdamW\n",
      "File \u001b[0;32m~/Documents/Projects/Arte/venv/lib/python3.11/site-packages/torch/__init__.py:1241\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m onnx \u001b[39mas\u001b[39;00m onnx\n\u001b[1;32m   1240\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m jit \u001b[39mas\u001b[39;00m jit\n\u001b[0;32m-> 1241\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m linalg \u001b[39mas\u001b[39;00m linalg\n\u001b[1;32m   1242\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m hub \u001b[39mas\u001b[39;00m hub\n\u001b[1;32m   1243\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m random \u001b[39mas\u001b[39;00m random\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1069\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:729\u001b[0m, in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## IMPORTS\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForTokenClassification, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tokens': ['2022', 'óleo', 'sobre', 'tela'], 'labels': ['O', 'O', 'O', 'O', 'B-LABEL', 'I-LABEL', 'I-LABEL']}, {'tokens': ['Obra', 'faz', 'parte', 'da', 'série', 'Enquanto', 'isso,', 'lá', 'foraRealizada', 'sobre', 'painel', 'telado', 'com', 'carvão', 'e', 'cera', 'de', 'abelhaEmoldurada', 'em', 'madeira', 'preta'], 'labels': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LABEL', 'I-LABEL', 'I-LABEL', 'I-LABEL', 'I-LABEL', 'I-LABEL', 'I-LABEL', 'I-LABEL']}, {'tokens': ['2022', 'Óleo', 'sobre', 'tela', 'alo', 'eu', 'sou', 'legal'], 'labels': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LABEL', 'I-LABEL', 'I-LABEL', 'O']}, {'tokens': ['Mulheres', 'do', 'Bordel,', 'déc.', '1950.', 'Aquarela', 'e', 'nanquim', 'sobre', 'cartão.'], 'labels': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LABEL', 'I-LABEL', 'I-LABEL', 'I-LABEL', 'I-LABEL', 'O']}]\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {\"text\": \"2022 óleo sobre tela\", \"label\": \"óleo sobre tela\"},\n",
    "    {\"text\": \"Obra faz parte da série Enquanto isso, lá foraRealizada sobre painel telado com carvão e cera de abelha Emoldurada em madeira preta\", \"label\": \"carvão e cera de abelha sobre painel telado\"},\n",
    "    {\"text\": \"2022 Óleo sobre tela alo eu sou legal\", \"label\": \"óleo sobre tela\"},\n",
    "    {\"text\": \"Mulheres do Bordel, déc. 1950. Aquarela e nanquim sobre cartão.\", \"label\": \"aquarela e nanquim sobre cartão\"}\n",
    "]\n",
    "\n",
    "def convert_to_iob_format(data):\n",
    "    iob_data = []\n",
    "    for entry in data:\n",
    "        text = entry['text']\n",
    "        label = entry['label']\n",
    "        \n",
    "        tokens = text.split()\n",
    "        label_tokens = label.split()\n",
    "        \n",
    "        iob_labels = ['O'] * len(tokens)\n",
    "        \n",
    "        # Find the start and end positions of the label in the text\n",
    "        start_pos = text.find(label_tokens[0])\n",
    "        end_pos = start_pos + len(label_tokens[0])\n",
    "        \n",
    "        # Mark the tokens within the label as 'B-LABEL' and 'I-LABEL'\n",
    "        iob_labels[start_pos:end_pos] = ['B-LABEL'] + ['I-LABEL'] * (len(label_tokens) - 1)\n",
    "        \n",
    "        # Create a new dictionary with IOB-formatted labels\n",
    "        iob_entry = {\n",
    "            'tokens': tokens,\n",
    "            'labels': iob_labels\n",
    "        }\n",
    "        \n",
    "        iob_data.append(iob_entry)\n",
    "    \n",
    "    return iob_data\n",
    "\n",
    "iob_data = convert_to_iob_format(data)\n",
    "print(iob_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materials (text1): []\n",
      "Mediums (text1): []\n",
      "Materials (text2): []\n",
      "Mediums (text2): []\n"
     ]
    }
   ],
   "source": [
    "# Define the labeled dataset and preprocessing functions\n",
    "# ...\n",
    "\n",
    "# Create a custom dataset for PyTorch DataLoader\n",
    "class ArtworkDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# Define the BERT-based NER model\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-multilingual-cased', num_labels=num_labels)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Define your DataLoader with the custom dataset\n",
    "train_dataset = ArtworkDataset(train_texts, train_labels, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "# ...\n",
    "\n",
    "# Test the model\n",
    "# ...\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
