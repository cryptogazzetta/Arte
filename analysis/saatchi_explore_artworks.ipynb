{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "\n",
    "# External modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "# Models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, median_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "# Project modules\n",
    "import filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTIONS\n",
    "\n",
    "\n",
    "## DATAFRAME CREATION\n",
    "def get_artworks_df():\n",
    "    artworks = pd.read_csv('../temporary-files/saatchi_artworks_info.csv')\n",
    "    artworks.rename(columns=lambda x: x.title(), inplace=True)\n",
    "    artworks.rename(columns={'Price': 'Price (US$)', 'Size': 'Size (in²)'}, inplace=True)\n",
    "    artworks = calculate_area_and_price_per_area(artworks)\n",
    "    for column_name in ['Styles', 'Mediums', 'Subjects', 'Materials']:\n",
    "        artworks[column_name] = artworks[column_name].apply(lambda x: [str(value.strip()) for value in x.split(',')])\n",
    "\n",
    "    # Filter out non 'one-of-a-kind' artworks\n",
    "    artworks = artworks[artworks['Original'] == 'One-of-a-kind Artwork']\n",
    "\n",
    "    for column_name in ['Title']:\n",
    "        artworks[column_name] = artworks[column_name].apply(lambda x: [str(value.strip()) for value in x.split(' ')])\n",
    "    # Remove outliers (artworks with Price (US$/in²) in the 5% and 95% percentiles)\n",
    "    artworks = artworks[artworks['Price (US$/in²)'] > artworks['Price (US$/in²)'].quantile(0.05)]\n",
    "    artworks = artworks[artworks['Price (US$/in²)'] < artworks['Price (US$/in²)'].quantile(0.95)]\n",
    "    return artworks\n",
    "\n",
    "\n",
    "def calculate_area_and_price_per_area(dataframe):\n",
    "    # Iterate over the \"Size\" column\n",
    "    df = dataframe.copy()\n",
    "    for i, size in enumerate(dataframe['Size (in²)']):\n",
    "        # Extract the dimensions using regular expression\n",
    "        dimensions = re.findall(r'\\d+(?:\\.\\d+)?', size)\n",
    "        if len(dimensions) >= 2:\n",
    "            # Extract the width and height dimensions\n",
    "            try:\n",
    "                width = float(dimensions[0])\n",
    "                height = float(dimensions[1])\n",
    "                total_area = width * height\n",
    "                df.at[i, 'Size (in²)'] = total_area\n",
    "            except:\n",
    "                df.at[i, 'Size (in²)'] = 'NaN'\n",
    "    df['Price (US$/in²)'] = df['Price (US$)'] / df['Size (in²)']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_occurrence_count_on_col_dict(values):\n",
    "    # Get all unique values from the column\n",
    "    occurrences_counts_dict = dict(Counter(values))\n",
    "    return occurrences_counts_dict\n",
    "\n",
    "\n",
    "def create_segments_dataframe(segments_dfs):\n",
    "    all_segments_df = pd.DataFrame(index=segments_dfs.keys(),\n",
    "                                   columns=['Mean Price (US$)', 'Median Price (US$)', 'Mean Size (in²)','Median Size (in²)',\n",
    "                                            'Mean Price (US$/in²)', 'Median Price (US$/in²)', 'Count'])\n",
    "    for key, value in segments_dfs.items():\n",
    "        all_segments_df.loc[key, 'Mean Price (US$)'] = value['Price (US$)'].mean().round(0)\n",
    "        all_segments_df.loc[key, 'Median Price (US$)'] = value['Price (US$)'].median().round(0)\n",
    "        all_segments_df.loc[key, 'Mean Size (in²)'] = value['Size (in²)'].mean().round(0)\n",
    "        all_segments_df.loc[key, 'Median Size (in²)'] = value['Size (in²)'].median().round(0)\n",
    "        all_segments_df.loc[key, 'Mean Price (US$/in²)'] = value['Price (US$/in²)'].mean().round(2)\n",
    "        all_segments_df.loc[key, 'Median Price (US$/in²)'] = value['Price (US$/in²)'].median().round(2)\n",
    "        all_segments_df.loc[key, 'Count'] = len(value)\n",
    "    all_segments_df.sort_values(by='Mean Price (US$)', ascending=False, inplace=True)\n",
    "    return all_segments_df\n",
    "\n",
    "\n",
    "def group_by_segments(artworks_data, column_name, column, occurrences_threshold):\n",
    "\n",
    "    segments_dfs = get_dfs_for_segments(filtered_artworks_data, column_name, occurrence_count_on_col_dict, occurrences_threshold)\n",
    "\n",
    "    create_segments_dataframe(segments_dfs)\n",
    "    \n",
    "    # New dataframe for each segment\n",
    "    segments_dfs = {}\n",
    "    for key, value in occurrence_count_on_col_dict.items():\n",
    "        if value > occurrences_threshold:\n",
    "            segments_dfs[key] = dataframe[dataframe[column_name].apply(lambda x: key in x)]\n",
    "    return segments_dfs\n",
    "\n",
    "\n",
    "\n",
    "def analyse_by_column(dataframe, column_name, threshold):\n",
    "    artworks_count_by_segment = dataframe[column_name].value_counts()\n",
    "    artworks_count_pct_by_segment = artworks_count_by_segment / dataframe[column_name].value_counts().sum()\n",
    "    selection = artworks_count_by_segment[artworks_count_by_segment > threshold].index\n",
    "    dataframe = dataframe[dataframe[column_name].isin(selection)]\n",
    "    return dataframe\n",
    "\n",
    "def compare_segments(dataframe, segments_to_compare, x_column_name, y_column_name):\n",
    "    for segment in segments_to_compare:\n",
    "        print(segment)\n",
    "\n",
    "        if segment == 'All':\n",
    "            segment_df = dataframe\n",
    "        else:\n",
    "            segment_df = segments_dfs[segment]\n",
    "\n",
    "        x = segment_df[[x_column_name]]\n",
    "        y = segment_df[y_column_name]\n",
    "        print('stats:', get_stats(segment_df, x, y))\n",
    "        get_all_models(x, y)\n",
    "\n",
    "        xlim = (0, 10000)\n",
    "        ylim = (0, 40000)\n",
    "\n",
    "\n",
    "def segment_and_clean_data(artworks_data, column_name, occurrences_threshold):\n",
    "    column = artworks_data[column_name]\n",
    "    \n",
    "    segments_in_column_list = [value for sublist in column for value in sublist]\n",
    "    occurrence_count_on_col_dict = get_occurrence_count_on_col_dict(segments_in_column_list)\n",
    "    \n",
    "    filtered_artworks_data = remove_empty_rows(artworks_data, column)\n",
    "    filtered_artworks_data = filtered_artworks_data.dropna(subset=['Price (US$)', 'Size (in²)'])\n",
    "    filtered_artworks_data[column_name] = column\n",
    "    \n",
    "    segments_dfs = get_dataframes_for_segments(filtered_artworks_data, column_name, occurrence_count_on_col_dict, occurrences_threshold)\n",
    "    all_segments_df = create_segments_dataframe(segments_dfs)\n",
    "    \n",
    "    return filtered_artworks_data, all_segments_df, segments_dfs\n",
    "\n",
    "\n",
    "def remove_empty_rows(dataframe, column):\n",
    "    return dataframe[column.apply(lambda x: len(x) > 0)]\n",
    "\n",
    "\n",
    "def get_dataframes_for_segments(dataframe, column_name, occurrence_count_on_col_dict, occurrences_threshold):\n",
    "    segments_dfs = {}\n",
    "    for key, value in occurrence_count_on_col_dict.items():\n",
    "        if value > occurrences_threshold:\n",
    "            segments_dfs[key] = dataframe[dataframe[column_name].apply(lambda x: key in x)]\n",
    "    return segments_dfs\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataframe_dummies(artworks_data, column_name, segments_dfs):\n",
    "    dummies_for_segment = artworks_data[['Price (US$)', column_name]].dropna(subset=['Price (US$)'])\n",
    "    \n",
    "    for key, value in segments_dfs.items():\n",
    "        dummies_for_segment[key] = artworks_data[column_name].apply(lambda x: True if key in x else False)\n",
    "    \n",
    "    return dummies_for_segment\n",
    "\n",
    "\n",
    "def get_dummies_for_all_segments(artworks):\n",
    "    dummies_for_all_segments = pd.DataFrame()\n",
    "\n",
    "    # get one df with dummies for styles, mediums, materials and subjects\n",
    "    for column_name in ['Styles', 'Mediums', 'Materials', 'Subjects']:\n",
    "        column = artworks[column_name]\n",
    "        occurrences_threshold = 200\n",
    "        filtered_artworks_data, all_segments_df, segments_dfs = segment_and_clean_data(artworks, column_name, occurrences_threshold)\n",
    "        dummies_for_segment = prepare_dataframe_dummies(artworks, column_name, segments_dfs).drop(columns=['Price (US$)'])\n",
    "        # concat dummies_for_segment to dummies_for_all_segments\n",
    "        dummies_for_all_segments = pd.concat([dummies_for_all_segments, dummies_for_segment], axis=1)\n",
    "\n",
    "\n",
    "    dummies_for_all_segments.drop(columns=['Styles', 'Mediums', 'Materials', 'Subjects'], inplace=True)\n",
    "\n",
    "    # add price, size, country\n",
    "    columns_to_add = ['Size (in²)', 'Price (US$)', 'Price (US$/in²)', 'Country', 'Favorite', 'Url', 'Title', 'Artist Produced Limited Edition Of', 'Original Created']\n",
    "    for column_name in columns_to_add:\n",
    "        dummies_for_all_segments[column_name] = artworks[column_name]\n",
    "\n",
    "    return dummies_for_all_segments\n",
    "\n",
    "\n",
    "def analyse_by_segments(artworks, column_name, sortby, min_frequency):\n",
    "    artworks_by_segment = analyse_by_column(artworks, column_name1, min_frequency1)\n",
    "    # Get mean price per segment as 'Mean Price' column\n",
    "    mean_price_by_segment = artworks_by_segment.groupby(column_name1).agg({'Price (US$)': 'mean'})\n",
    "    median_price_by_segment = artworks_by_segment.groupby(column_name1).agg({'Price (US$)': 'median'})\n",
    "    mean_price_per_in_by_segment = artworks_by_segment.groupby(column_name1).agg({'Price (US$/in²)': 'mean'})\n",
    "    median_price_per_in_by_segment = artworks_by_segment.groupby(column_name1).agg({'Price (US$/in²)': 'median'})\n",
    "    col_analysis_by_segment = pd.concat([mean_price_by_segment, median_price_by_segment, mean_price_per_in_by_segment, median_price_per_in_by_segment, artworks[column_name1].value_counts()], axis=1)\n",
    "    col_analysis_by_segment.columns = ['Mean Price (US$)', 'Median Price (US$)','Mean Price (US$/in²)', 'Median Price (US$/in²)', 'Count']\n",
    "    # filter out segments with less than [min_frequency] artworks\n",
    "    col_analysis_by_segment = col_analysis_by_segment[col_analysis_by_segment['Count'] > min_frequency1]\n",
    "\n",
    "    col_analysis_by_segment.sort_values(by=sort_by1, ascending=False, inplace=True)\n",
    "    col_analysis_by_segment = col_analysis_by_segment.dropna()\n",
    "\n",
    "    return col_analysis_by_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODELS\n",
    "\n",
    "\n",
    "def get_stats(x, y):\n",
    "    stats = {}\n",
    "    for variable in [x, y]:\n",
    "        max_variable = round(max(variable), 2)\n",
    "        min_variable = round(min(variable), 2)\n",
    "        mean_variable = round(variable.mean(), 2)\n",
    "        median_variable = round(variable.median(), 2)\n",
    "        stats_variable = {'Max': max_variable, 'Min': min_variable, 'Mean': mean_variable, 'Median': median_variable}\n",
    "        # add stats_variable to stats\n",
    "        stats[variable] = stats_variable\n",
    "    return stats\n",
    "\n",
    "\n",
    "## GET MODELS\n",
    "\n",
    "def get_decision_tree(X_train, y_train):\n",
    "    decision_tree = DecisionTreeRegressor(random_state=42)\n",
    "    decision_tree.fit(X_train, y_train)\n",
    "    return decision_tree\n",
    "\n",
    "def get_linear_regression(X_train, y_train):\n",
    "    linear_regression = LinearRegression()\n",
    "    linear_regression.fit(X_train, y_train)\n",
    "    return linear_regression\n",
    "\n",
    "def get_random_forest(X_train, y_train):\n",
    "    rf_model = RandomForestRegressor(random_state=1)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    return rf_model\n",
    "\n",
    "def get_gradient_boosting(X_train, y_train):\n",
    "    # get gradient boosting model\n",
    "    gb_model = GradientBoostingRegressor(random_state=1)\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    return gb_model\n",
    "\n",
    "# gives a dataframe taking models as columns and score as rows\n",
    "def get_all_models(x, y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1)\n",
    "\n",
    "    models = {'Linear regression': get_linear_regression(x_train, y_train),\n",
    "              'Decision tree': get_decision_tree(x_train, y_train),\n",
    "              'Random forest': get_random_forest(x_train, y_train),\n",
    "              'Gradient boosting': get_gradient_boosting(x_train, y_train)}\n",
    "    \n",
    "    models_df = pd.DataFrame(columns=models.keys())\n",
    "    for model_name, model in models.items():\n",
    "        y_pred = model.predict(x_test)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        n = len(y_test)\n",
    "        p = x_test.shape[1]\n",
    "        adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "        mean_error = mean_absolute_error(y_test, y_pred)\n",
    "        median_error = median_absolute_error(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        pearson = np.corrcoef(y_test.to_numpy(), y_pred)[0, 1]\n",
    "\n",
    "        models_df.loc['Pearson Correl Predicted-Actual', model_name] = pearson\n",
    "        models_df.loc['R² Score', model_name] = r2\n",
    "        models_df.loc['Adjusted R² Score', model_name] = adj_r2\n",
    "        models_df.loc['Mean Absolute Error', model_name] = mean_error\n",
    "        models_df.loc['Median Absolute Error', model_name] = median_error\n",
    "        models_df.loc['Mean Absolute Percentage Error', model_name] = mape\n",
    "        models_df.loc['Mean Squared Error', model_name] = mse\n",
    "\n",
    "        # Round values\n",
    "        models_df = models_df.round(2)\n",
    "\n",
    "    return models_df, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHARTS\n",
    "\n",
    "dark_green_color_code = '#195921'\n",
    "gold_color_code = '#FFD700'\n",
    "black_color_code = '#000000'\n",
    "salmon_color_code = '#FA8072'\n",
    "\n",
    "def plot_segment_chart(dataframe, segments_column_name, bar_column_name, line_columns_names, title):\n",
    "    bar_color_code = dark_green_color_code\n",
    "    line_colors_codes = [gold_color_code, black_color_code]\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.bar(dataframe.index, dataframe[bar_column_name], color=bar_color_code)\n",
    "    for line_column_name in line_columns_names:\n",
    "        ax2.plot(dataframe.index, dataframe[line_column_name], label=line_column_name, color=line_colors_codes.pop(0))\n",
    "    ax2.legend()\n",
    "    ax1.set_xticklabels(dataframe.index, rotation=90)\n",
    "    ax1.set_ylabel(bar_column_name)\n",
    "    ax2.set_ylabel(str(line_columns_names))\n",
    "    ax1.set_xlabel(segments_column_name)\n",
    "    ax1.set_title(title)\n",
    "    # beautify and enhance readability of the chart\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "\n",
    "    ax1.tick_params(axis='x', which='major', labelsize=8)\n",
    "    ax1.tick_params(axis='y', which='major', labelsize=8)\n",
    "    ax2.tick_params(axis='y', which='major', labelsize=8)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# SCATTER PLOT: SIZE VS PRICE\n",
    "def plot_scatter(dataframe, x_column_name, y_columnname, title):\n",
    "    plt.scatter(x=dataframe[x_column_name], y=dataframe[y_columnname], c=dark_green_color_code)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_column_name)\n",
    "    plt.ylabel(y_columnname)\n",
    "    # improve readability of the chart\n",
    "    plt.tick_params(axis='x', which='major', labelsize=8)\n",
    "    plt.tick_params(axis='y', which='major', labelsize=8)\n",
    "    # set color of scatter points as dark green\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'Packaging', 'Collage', 'Multi-Paneled Collage', 'Mediums',\n",
       "       'Ready To Hang', 'Original', 'Digital', 'Country',\n",
       "       'Multi-Paneled Mixed Media', 'Number Of Pieces', 'Artist_Link',\n",
       "       'Styles', 'Subjects', 'Multi-Paneled Printmaking', 'Mixed Media',\n",
       "       'Multi-Paneled Painting', 'Handling', 'Installation', 'Delivery Time',\n",
       "       'Number Of Panels', 'Sale_Status', 'Materials', 'Photography', 'Url',\n",
       "       'Artist Produced Limited Edition Of', 'Img', 'Ships From', 'Artist',\n",
       "       'Multi-Paneled Sculpture', 'Favorite', 'Multi-Paneled Installation',\n",
       "       'Original Created', 'Price (US$)', 'Sculpture',\n",
       "       'Multi-Paneled Photography', 'Customs', 'Views', 'Description', 'Frame',\n",
       "       'Printmaking', 'Multi-Paneled Drawing', 'Painting', 'Drawing',\n",
       "       'Size (in²)', 'Price (US$/in²)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artworks = get_artworks_df()\n",
    "# artworks = artworks[['Styles', 'Mediums', 'Subjects', 'Artist', 'Size', 'Price']]\n",
    "artworks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Linear regression</th>\n",
       "      <th>Decision tree</th>\n",
       "      <th>Random forest</th>\n",
       "      <th>Gradient boosting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pearson Correl Predicted-Actual</th>\n",
       "      <td>0.76682</td>\n",
       "      <td>0.730919</td>\n",
       "      <td>0.761335</td>\n",
       "      <td>0.782786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R² Score</th>\n",
       "      <td>0.586808</td>\n",
       "      <td>0.511749</td>\n",
       "      <td>0.574501</td>\n",
       "      <td>0.61265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adjusted R² Score</th>\n",
       "      <td>0.58667</td>\n",
       "      <td>0.511586</td>\n",
       "      <td>0.574359</td>\n",
       "      <td>0.612521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean Absolute Error</th>\n",
       "      <td>1249.062655</td>\n",
       "      <td>1225.653147</td>\n",
       "      <td>1192.104353</td>\n",
       "      <td>1203.711335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Median Absolute Error</th>\n",
       "      <td>673.491157</td>\n",
       "      <td>587.95288</td>\n",
       "      <td>583.332725</td>\n",
       "      <td>655.760439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean Absolute Percentage Error</th>\n",
       "      <td>0.680983</td>\n",
       "      <td>0.56141</td>\n",
       "      <td>0.550329</td>\n",
       "      <td>0.59034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean Squared Error</th>\n",
       "      <td>4847660.67904</td>\n",
       "      <td>5728270.050108</td>\n",
       "      <td>4992050.956775</td>\n",
       "      <td>4544479.998509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Linear regression   Decision tree  \\\n",
       "Pearson Correl Predicted-Actual           0.76682        0.730919   \n",
       "R² Score                                 0.586808        0.511749   \n",
       "Adjusted R² Score                         0.58667        0.511586   \n",
       "Mean Absolute Error                   1249.062655     1225.653147   \n",
       "Median Absolute Error                  673.491157       587.95288   \n",
       "Mean Absolute Percentage Error           0.680983         0.56141   \n",
       "Mean Squared Error                  4847660.67904  5728270.050108   \n",
       "\n",
       "                                  Random forest Gradient boosting  \n",
       "Pearson Correl Predicted-Actual        0.761335          0.782786  \n",
       "R² Score                               0.574501           0.61265  \n",
       "Adjusted R² Score                      0.574359          0.612521  \n",
       "Mean Absolute Error                 1192.104353       1203.711335  \n",
       "Median Absolute Error                583.332725        655.760439  \n",
       "Mean Absolute Percentage Error         0.550329           0.59034  \n",
       "Mean Squared Error               4992050.956775    4544479.998509  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## SIZE AND PRICE\n",
    "\n",
    "size_x = artworks[['Size (in²)']]\n",
    "y = artworks['Price (US$)']\n",
    "\n",
    "get_all_models(size_x, y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m x \u001b[39m=\u001b[39m dummies_for_all_segments\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mPrice (US$)\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPrice (US$/in²)\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCountry\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFavorite\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mUrl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTitle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mOriginal Created\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m y \u001b[39m=\u001b[39m dummies_for_all_segments[\u001b[39m'\u001b[39m\u001b[39mPrice (US$)\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> 9\u001b[0m models_output \u001b[39m=\u001b[39m get_all_models(x\u001b[39m.\u001b[39;49mvalues, y)\n\u001b[0;32m     10\u001b[0m models_df \u001b[39m=\u001b[39m models_output[\u001b[39m0\u001b[39m]\n\u001b[0;32m     11\u001b[0m models \u001b[39m=\u001b[39m models_output[\u001b[39m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[9], line 44\u001b[0m, in \u001b[0;36mget_all_models\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_all_models\u001b[39m(x, y):\n\u001b[0;32m     42\u001b[0m     x_train, x_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(x, y, random_state\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 44\u001b[0m     models \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mLinear regression\u001b[39m\u001b[39m'\u001b[39m: get_linear_regression(x_train, y_train),\n\u001b[0;32m     45\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mDecision tree\u001b[39m\u001b[39m'\u001b[39m: get_decision_tree(x_train, y_train),\n\u001b[0;32m     46\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mRandom forest\u001b[39m\u001b[39m'\u001b[39m: get_random_forest(x_train, y_train),\n\u001b[0;32m     47\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mGradient boosting\u001b[39m\u001b[39m'\u001b[39m: get_gradient_boosting(x_train, y_train)}\n\u001b[0;32m     49\u001b[0m     models_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39mmodels\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m     50\u001b[0m     \u001b[39mfor\u001b[39;00m model_name, model \u001b[39min\u001b[39;00m models\u001b[39m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[9], line 26\u001b[0m, in \u001b[0;36mget_linear_regression\u001b[1;34m(X_train, y_train)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_linear_regression\u001b[39m(X_train, y_train):\n\u001b[0;32m     25\u001b[0m     linear_regression \u001b[39m=\u001b[39m LinearRegression()\n\u001b[1;32m---> 26\u001b[0m     linear_regression\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     27\u001b[0m     \u001b[39mreturn\u001b[39;00m linear_regression\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_base.py:678\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    674\u001b[0m n_jobs_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs\n\u001b[0;32m    676\u001b[0m accept_sparse \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositive \u001b[39melse\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcoo\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> 678\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    679\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39;49maccept_sparse, y_numeric\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m    680\u001b[0m )\n\u001b[0;32m    682\u001b[0m has_sw \u001b[39m=\u001b[39m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m has_sw:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    619\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    622\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1142\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1144\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1145\u001b[0m     )\n\u001b[1;32m-> 1147\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1148\u001b[0m     X,\n\u001b[0;32m   1149\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1150\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1151\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1152\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1153\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1154\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1155\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1156\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1157\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1158\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1159\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1160\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[0;32m   1163\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m         )\n\u001b[0;32m    958\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 959\u001b[0m         _assert_all_finite(\n\u001b[0;32m    960\u001b[0m             array,\n\u001b[0;32m    961\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    962\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    963\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    964\u001b[0m         )\n\u001b[0;32m    966\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    125\u001b[0m     X,\n\u001b[0;32m    126\u001b[0m     xp\u001b[39m=\u001b[39;49mxp,\n\u001b[0;32m    127\u001b[0m     allow_nan\u001b[39m=\u001b[39;49mallow_nan,\n\u001b[0;32m    128\u001b[0m     msg_dtype\u001b[39m=\u001b[39;49mmsg_dtype,\n\u001b[0;32m    129\u001b[0m     estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    130\u001b[0m     input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    131\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    157\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[1;32m--> 173\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "## MODEL WITH DUMMIES FOR SEGMENTS (STYLES, MEDIUMS, MATERIALS AND SUBJECTS)\n",
    "\n",
    "dummies_for_all_segments = get_dummies_for_all_segments(artworks)\n",
    "\n",
    "# apply models to dummies_for_all_segments\n",
    "x = dummies_for_all_segments.drop(columns=['Price (US$)', 'Price (US$/in²)', 'Country', 'Favorite', 'Url', 'Title', 'Original Created'])\n",
    "y = dummies_for_all_segments['Price (US$)']\n",
    "\n",
    "models_output = get_all_models(x.values, y)\n",
    "models_df = models_output[0]\n",
    "models = models_output[1]\n",
    "models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ARTWORKS BY SEGMENT\n",
    "column_name1 = 'Country'\n",
    "sort_by1 = 'Count'\n",
    "min_frequency1 = artworks.shape[0] * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ARTWORKS BY SEGMENT\n",
    "\n",
    "col_analysis_by_segment = analyse_by_segments(artworks, column_name1, sort_by1, min_frequency1)\n",
    "\n",
    "plot_segment_chart(col_analysis_by_segment, column_name1, 'Count', [], 'Artworks by ' + column_name1)\n",
    "col_analysis_by_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHOOSE COLUMN TO EXPLORE\n",
    "artworks_data = get_artworks_df()\n",
    "\n",
    "column = artworks_data.Mediums\n",
    "occurrences_threshold = artworks_data.shape[0]/100\n",
    "sort_by = 'Median Price (US$/in²)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANALYSE SEGMENTS\n",
    "\n",
    "# TEMAS: E SE TIRAR ARTE ABSTRATA?\n",
    "\n",
    "column_name = column.name\n",
    "filtered_artworks_data, all_segments_df, segments_dfs = segment_and_clean_data(artworks_data, column_name, occurrences_threshold)\n",
    "dummies_for_segment = prepare_dataframe_dummies(artworks_data, column_name, segments_dfs)\n",
    "\n",
    "# get all models to dummies_for_segment\n",
    "x = dummies_for_segment.drop(['Price (US$)', column_name], axis=1)\n",
    "y = dummies_for_segment['Price (US$)']\n",
    "segment_models_df, segment_models = get_all_models(x, y)\n",
    "\n",
    "all_segments_df.sort_values(by=sort_by, ascending=False, inplace=True)\n",
    "\n",
    "plot_segment_chart(all_segments_df, column_name, 'Count', ['Mean Price (US$/in²)', 'Median Price (US$/in²)'], 'Artworks by ' + column_name)\n",
    "all_segments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DESCRITIVE ANALYSIS OF COLUMN\n",
    "\n",
    "column = artworks['Price (US$)']\n",
    "\n",
    "descritive_df = pd.DataFrame()\n",
    "descritive_df.loc[0, 'max'] = column.max()\n",
    "descritive_df.loc[0, 'min'] = column.min()\n",
    "descritive_df.loc[0, 'mean'] = column.mean()\n",
    "descritive_df.loc[0, 'median'] = column.median()\n",
    "descritive_df.loc[0, 'stdev'] = column.std()\n",
    "descritive_df.loc[0, 'var'] = column.var()\n",
    "\n",
    "column.hist(color='black', alpha=0.5)\n",
    "plt.title('Histogram of '+column.name)\n",
    "plt.xlabel(column.name)\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "descritive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODELS FOR SEGMENT\n",
    "\n",
    "segment_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Linear regression</th>\n",
       "      <th>Decision tree</th>\n",
       "      <th>Random forest</th>\n",
       "      <th>Gradient boosting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pearson Correl Predicted-Actual</th>\n",
       "      <td>0.784326</td>\n",
       "      <td>0.698503</td>\n",
       "      <td>0.827543</td>\n",
       "      <td>0.812593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R² Score</th>\n",
       "      <td>0.614601</td>\n",
       "      <td>0.345804</td>\n",
       "      <td>0.684098</td>\n",
       "      <td>0.65995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adjusted R² Score</th>\n",
       "      <td>0.60753</td>\n",
       "      <td>0.3338</td>\n",
       "      <td>0.678301</td>\n",
       "      <td>0.653711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean Absolute Error</th>\n",
       "      <td>1216.260818</td>\n",
       "      <td>1285.230475</td>\n",
       "      <td>976.324545</td>\n",
       "      <td>1114.082508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Median Absolute Error</th>\n",
       "      <td>702.755337</td>\n",
       "      <td>397.0</td>\n",
       "      <td>400.70494</td>\n",
       "      <td>598.392571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean Absolute Percentage Error</th>\n",
       "      <td>0.6674</td>\n",
       "      <td>0.527002</td>\n",
       "      <td>0.435825</td>\n",
       "      <td>0.555816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean Squared Error</th>\n",
       "      <td>4521587.693942</td>\n",
       "      <td>7675176.852497</td>\n",
       "      <td>3706236.856382</td>\n",
       "      <td>3989543.435572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Linear regression   Decision tree  \\\n",
       "Pearson Correl Predicted-Actual          0.784326        0.698503   \n",
       "R² Score                                 0.614601        0.345804   \n",
       "Adjusted R² Score                         0.60753          0.3338   \n",
       "Mean Absolute Error                   1216.260818     1285.230475   \n",
       "Median Absolute Error                  702.755337           397.0   \n",
       "Mean Absolute Percentage Error             0.6674        0.527002   \n",
       "Mean Squared Error                 4521587.693942  7675176.852497   \n",
       "\n",
       "                                  Random forest Gradient boosting  \n",
       "Pearson Correl Predicted-Actual        0.827543          0.812593  \n",
       "R² Score                               0.684098           0.65995  \n",
       "Adjusted R² Score                      0.678301          0.653711  \n",
       "Mean Absolute Error                  976.324545       1114.082508  \n",
       "Median Absolute Error                 400.70494        598.392571  \n",
       "Mean Absolute Percentage Error         0.435825          0.555816  \n",
       "Mean Squared Error               3706236.856382    3989543.435572  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## MODEL FOR SIZE, STYLES, MEDIUMS, MATERIALS AND SUBJECTS\n",
    "\n",
    "# apply models to dummies_for_all_segments\n",
    "x = dummies_for_all_segments.drop(columns=['Price (US$)', 'Price (US$/in²)', 'Country', 'Favorite', 'Url', 'Title', 'Artist Produced Limited Edition Of', 'Original Created'])\n",
    "y = dummies_for_all_segments['Price (US$)']\n",
    "\n",
    "models_output = get_all_models(x.values, y)\n",
    "models_df = models_output[0]\n",
    "models = models_output[1]\n",
    "models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION MATRIX BETWEEN DUMMIES\n",
    "\n",
    "data = dummies_for_all_segments.drop(columns=['Price (US$)', 'Price (US$/in²)', 'Country', 'Favorite', 'Url', 'Title', 'Artist Produced Limited Edition Of', 'Original Created'])\n",
    "correlation_matrix = data.corr()\n",
    "correlation_matrix\n",
    "# show only correlations above 0.5 but different than 1\n",
    "correlation_matrix[(correlation_matrix > 0.5) & (correlation_matrix < 1)]\n",
    "# show correlations between 0.5 and 1 (without duplicates)\n",
    "correlation_matrix[(correlation_matrix > 0.5) & (correlation_matrix < 1)].stack().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET FEATURE IMPORTANCES\n",
    "\n",
    "rf_model = models['Random forest']\n",
    "\n",
    "feature_importances = pd.DataFrame(rf_model.feature_importances_,\n",
    "                                      index = x.columns,\n",
    "                                        columns=['importance']).sort_values('importance', ascending=False)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DISTRIBUTION OF RESIDUALS\n",
    "\n",
    "# get y_train and y_pred\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1)\n",
    "y_pred = rf_model.predict(x_test)\n",
    "\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Step 3: Visualize the distribution of residuals using a histogram\n",
    "plt.hist(residuals, bins=20, color=dark_green_color_code)\n",
    "# line of the mean\n",
    "plt.axvline(residuals.mean(), color='gold', linestyle='dashed', linewidth=1)\n",
    "plt.xlabel(\"Residuals (True Values - Predicted Values)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Residuals\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## distribuição por percentis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUANTILE LOSS\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def quantile_loss(y_true, y_pred, quantile):\n",
    "    errors = y_true - y_pred\n",
    "    return np.maximum(quantile * errors, (quantile - 1) * errors)\n",
    "\n",
    "quantiles = [0.1, 0.5, 0.9]\n",
    "\n",
    "for quantile in quantiles:\n",
    "    loss = quantile_loss(y_test, y_pred, quantile)\n",
    "    quantile_loss_value = np.mean(loss)\n",
    "    print(f\"Quantile Loss ({quantile}): {quantile_loss_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT PRICE VS SIZE\n",
    "\n",
    "## adicionar unidades\n",
    "## log x log\n",
    "\n",
    "plt.scatter(x=np.log(dummies_for_all_segments['Size'].astype(float)), y=np.log(dummies_for_all_segments['Price'].astype(float)), c=dark_green_color_code)\n",
    "plt.title('Size vs Price')\n",
    "plt.xlabel('Size (in²)')\n",
    "plt.ylabel('Price (US$)')\n",
    "# improve readability of the chart\n",
    "plt.tick_params(axis='x', which='major', labelsize=8)\n",
    "plt.tick_params(axis='y', which='major', labelsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ESTUDOS DE CASO:\n",
    "# QUADROS MAIS CAROS\n",
    "# MAIORES RESÍDUOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT Y_TEST VS Y_PRED\n",
    "\n",
    "# divide data into 3 percentils: 0-25%, 25-75%, 75-100%\n",
    "\n",
    "\n",
    "# plot results of the random forest model (y_pred) vs actual data (y_test)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1)\n",
    "rf_model = models['Random forest']\n",
    "y_pred = rf_model.predict(x_test)\n",
    "test_df = pd.DataFrame({'Actual Price (US$)': y_test, 'Predicted Price (US$)': y_pred})\n",
    "\n",
    "## adicionar linha identidade e pegar a inclinação dela\n",
    "\n",
    "## analisar todos os mais distantes da reta\n",
    "\n",
    "\n",
    "plot_scatter(test_df, 'Actual Price (US$)', 'Predicted Price (US$)', 'Actual price vs predicted price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PRICE AN ARTWORK BASED ON ITS CHARACTERISTICS\n",
    "\n",
    "\n",
    "# Define characteristics\n",
    "size = 20000\n",
    "styles = ['Photorealism']\n",
    "mediums = ['Oil', 'Acrylic']\n",
    "materials = ['Canvas']\n",
    "subjects = ['Women']\n",
    "\n",
    "model_name = 'Random forest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET PRICE ESTIMATE\n",
    "\n",
    "model = models[model_name]\n",
    "\n",
    "# get columns names\n",
    "columns_names = x.columns\n",
    "\n",
    "mean_size = dummies_for_all_segments['Size (in²)'].mean()\n",
    "model_mean_error = models_df.loc['mean error', model_name]\n",
    "\n",
    "# get characteristics in json format\n",
    "characteristics = {'size': size, 'styles': styles, 'mediums': mediums, 'materials': materials, 'subjects': subjects}\n",
    "\n",
    "def get_df_for_model(characteristics):\n",
    "    test_df = pd.DataFrame(columns=columns_names)\n",
    "    test_df.loc[0, 'Size'] = characteristics['size']\n",
    "    keys = list(characteristics.keys())\n",
    "    keys.remove('size')\n",
    "    for criterium in keys:\n",
    "        for trait in characteristics[criterium]:\n",
    "            test_df.loc[0, trait] = True\n",
    "    # fill all other columns with False\n",
    "    test_df.fillna(False, inplace=True)\n",
    "    return test_df\n",
    "\n",
    "test_df = get_df_for_model(characteristics)\n",
    "\n",
    "price_estimate = model.predict(test_df)\n",
    "\n",
    "\n",
    "price_margin = min(model_mean_error * size / mean_size, model_mean_error)\n",
    "min_price = price_estimate - price_margin\n",
    "max_price = price_estimate + price_margin\n",
    "\n",
    "\n",
    "price_per_in2 = price_estimate / size\n",
    "print('Max Price:', max_price[0].round(2))\n",
    "print('Min Price:', min_price[0].round(2))\n",
    "print('Price per in²:', price_per_in2[0].round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## APPLYING MODELS TO SEGMENTS\n",
    "\n",
    "# multi variate linear regression with price as y and all segments as x\n",
    "x = dummies_for_segment.drop(['Price (US$)', column_name], axis=1)\n",
    "# dependent variable\n",
    "y = dummies_for_segment['Price (US$)']\n",
    "\n",
    "print('Models for',column_name ,'x','Price (US$)')\n",
    "get_all_models(x, y)\n",
    "print()\n",
    "print(dummies_for_segment.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPARE SEGMENTS:\n",
    "\n",
    "artworks_data = get_artworks_df()\n",
    "## COLUMN CHOSEN TO DEFINE SEGMENTS\n",
    "column_name = 'Styles'\n",
    "column = artworks_data.Styles\n",
    "occurrences_threshold = 0\n",
    "\n",
    "print('IN EACH LISTED SEGMENT, HOW STRONGLY DOES THE PRICE OF AN ARTWORK DEPEND ON ITS SIZE?')\n",
    "print()\n",
    "\n",
    "\n",
    "segments_dfs = group_by_segments(artworks_data, column_name, column, occurrences_threshold)\n",
    "\n",
    "\n",
    "segments_to_compare = ['Impressionism', 'Photorealism']\n",
    "\n",
    "# Variables\n",
    "x_column_name = 'Size (in²)'\n",
    "y_column_name = 'Price (US$)'\n",
    "\n",
    "filtered_artworks_data = artworks_data[artworks_data[column_name].apply(lambda x: any(item in x for item in segments_to_compare))]\n",
    "\n",
    "compare_segments(filtered_artworks_data, segments_to_compare, x_column_name, y_column_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
