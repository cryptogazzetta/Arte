{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "\n",
    "# External modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "# Models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, median_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "# ignorando warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Project modules\n",
    "import filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTIONS\n",
    "\n",
    "\n",
    "## DATAFRAME CREATION\n",
    "def get_artworks_df():\n",
    "    artworks = pd.read_csv('../temporary-files/saatchi_artworks_info.csv')\n",
    "    artworks.rename(columns=lambda x: x.title(), inplace=True)\n",
    "    artworks.rename(columns={'Price': 'Price (US$)', 'Size': 'Size (in²)'}, inplace=True)\n",
    "    artworks = calculate_area_and_price_per_area(artworks)\n",
    "\n",
    "    artworks.dropna(subset=['Price (US$)', 'Size (in²)'])\n",
    "    \n",
    "    for column_name in ['Styles', 'Mediums', 'Subjects', 'Materials']:\n",
    "        artworks[column_name] = artworks[column_name].apply(lambda x: [str(value.strip()) for value in x.split(',')])\n",
    "\n",
    "    # Filter out non 'one-of-a-kind' artworks\n",
    "    artworks = artworks[artworks['Original'] == 'One-of-a-kind Artwork']\n",
    "\n",
    "    # column Artist as single-item list\n",
    "    artworks['Artist'] = artworks['Artist'].apply(lambda x: [x])\n",
    "    \n",
    "    # Remove outliers (artworks with Price (US$/in²) in the 5% and 95% percentiles)\n",
    "    artworks = artworks[artworks['Price (US$/in²)'] > artworks['Price (US$/in²)'].quantile(0.05)]\n",
    "    artworks = artworks[artworks['Price (US$/in²)'] < artworks['Price (US$/in²)'].quantile(0.95)]\n",
    "    return artworks\n",
    "\n",
    "\n",
    "def calculate_area_and_price_per_area(dataframe):\n",
    "    # Iterate over the \"Size\" column\n",
    "    df = dataframe.copy()\n",
    "    for i, size in enumerate(dataframe['Size (in²)']):\n",
    "        # Extract the dimensions using regular expression\n",
    "        dimensions = re.findall(r'\\d+(?:\\.\\d+)?', size)\n",
    "        if len(dimensions) >= 2:\n",
    "            # Extract the width and height dimensions\n",
    "            try:\n",
    "                width = float(dimensions[0])\n",
    "                height = float(dimensions[1])\n",
    "                total_area = width * height\n",
    "                df.at[i, 'Size (in²)'] = total_area\n",
    "            except:\n",
    "                df.at[i, 'Size (in²)'] = 'NaN'\n",
    "    df['Price (US$/in²)'] = df['Price (US$)'] / df['Size (in²)']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_occurrence_count_on_col_dict(values):\n",
    "    # Get all unique values from the column\n",
    "    occurrences_counts_dict = dict(Counter(values))\n",
    "    return occurrences_counts_dict\n",
    "\n",
    "\n",
    "def create_segments_dataframe(segments_dfs):\n",
    "    all_segments_df = pd.DataFrame(index=segments_dfs.keys(),\n",
    "                                   columns=['Mean Price (US$/in²)', 'Median Price (US$/in²)',\n",
    "                                            'Standard Dev Price (US$/in²)', 'Variance (US$/in²)' 'Count (#)'])\n",
    "    for key, value in segments_dfs.items():\n",
    "        # all_segments_df.loc[key, 'Mean Price (US$)'] = value['Price (US$)'].mean().round(0)\n",
    "        # all_segments_df.loc[key, 'Median Price (US$)'] = value['Price (US$)'].median().round(0)\n",
    "        # all_segments_df.loc[key, 'Standard Dev Price (US$)'] = value['Price (US$)'].std().round(0)\n",
    "        # all_segments_df.loc[key, 'Variance (US$)'] = value['Price (US$)'].var().round(0)\n",
    "\n",
    "        all_segments_df.loc[key, 'Mean Price (US$/in²)'] = value['Price (US$/in²)'].mean().round(2)\n",
    "        all_segments_df.loc[key, 'Median Price (US$/in²)'] = value['Price (US$/in²)'].median().round(2)\n",
    "        all_segments_df.loc[key, 'Standard Dev Price (US$/in²)'] = value['Price (US$/in²)'].std().round(2)\n",
    "        all_segments_df.loc[key, 'Variance (US$/in²)'] = value['Price (US$/in²)'].var().round(2)\n",
    "\n",
    "        all_segments_df.loc[key, 'Count (#)'] = len(value)\n",
    "    all_segments_df.sort_values(by='Mean Price (US$/in²)', ascending=False, inplace=True)\n",
    "    return all_segments_df\n",
    "\n",
    "\n",
    "def analyse_by_column(dataframe, column_name, threshold):\n",
    "    artworks_count_by_segment = dataframe[column_name].value_counts()\n",
    "    artworks_count_pct_by_segment = artworks_count_by_segment / dataframe[column_name].value_counts().sum()\n",
    "    selection = artworks_count_by_segment[artworks_count_by_segment > threshold].index\n",
    "    dataframe = dataframe[dataframe[column_name].isin(selection)]\n",
    "    return dataframe\n",
    "\n",
    "def compare_segments(dataframe, segments_to_compare, x_column_name, y_column_name):\n",
    "    for segment in segments_to_compare:\n",
    "\n",
    "        if segment == 'All':\n",
    "            segment_df = dataframe\n",
    "        else:\n",
    "            segment_df = segments_dfs[segment]\n",
    "\n",
    "        x = segment_df[[x_column_name]]\n",
    "        y = segment_df[y_column_name]\n",
    "        print('stats:', get_stats(segment_df, x, y))\n",
    "        get_all_models(x, y)\n",
    "\n",
    "        xlim = (0, 10000)\n",
    "        ylim = (0, 40000)\n",
    "\n",
    "\n",
    "def get_segments_for_column(column):\n",
    "    segments = [value for sublist in column for value in sublist]\n",
    "    return segments\n",
    "\n",
    "\n",
    "def segment_and_clean_data(artworks_data, column_name, occurrences_threshold):\n",
    "    column = artworks_data[column_name]\n",
    "    \n",
    "    if column_name == 'Artist':\n",
    "        segments_in_column_list = artworks_data['Artist'].apply(lambda x: x[0]).to_list()\n",
    "    else:\n",
    "        segments_in_column_list = get_segments_for_column(column)\n",
    "    occurrence_count_on_col_dict = get_occurrence_count_on_col_dict(segments_in_column_list)\n",
    "    \n",
    "    filtered_artworks_data = remove_empty_rows(artworks_data, column)\n",
    "    filtered_artworks_data[column_name] = column\n",
    "    \n",
    "    segments_dfs = get_dataframes_for_segments(filtered_artworks_data, column_name, occurrence_count_on_col_dict, occurrences_threshold)\n",
    "    all_segments_df = create_segments_dataframe(segments_dfs)\n",
    "    \n",
    "    return filtered_artworks_data, all_segments_df, segments_dfs\n",
    "\n",
    "\n",
    "def remove_empty_rows(dataframe, column):\n",
    "    return dataframe[column.apply(lambda x: len(x) > 0)]\n",
    "\n",
    "\n",
    "def get_dataframes_for_segments(dataframe, column_name, occurrence_count_on_col_dict, occurrences_threshold):\n",
    "    segments_dfs = {}\n",
    "    for key, value in occurrence_count_on_col_dict.items():\n",
    "        if value > occurrences_threshold:\n",
    "            segments_dfs[key] = dataframe[dataframe[column_name].apply(lambda x: key in x)]\n",
    "    return segments_dfs\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataframe_dummies(artworks_data, column_name, segments_dfs):\n",
    "    dummies_for_segment = artworks_data[['Price (US$)', column_name]].dropna(subset=['Price (US$)'])\n",
    "    \n",
    "    for key, value in segments_dfs.items():\n",
    "        dummies_for_segment[key] = artworks_data[column_name].apply(lambda x: True if key in x else False)\n",
    "    \n",
    "    return dummies_for_segment\n",
    "\n",
    "\n",
    "def get_dummies_for_all_segments(artworks, columns_names):\n",
    "    dummies_for_all_segments = pd.DataFrame()\n",
    "\n",
    "    # get one df with dummies for styles, mediums, materials and subjects\n",
    "    for column_name in columns_names:\n",
    "        \n",
    "        if column_name == 'Artist':\n",
    "            occurrences_threshold = 5\n",
    "        else:\n",
    "            occurrences_threshold = artworks.shape[0] / 100\n",
    "        \n",
    "        segments_dfs = segment_and_clean_data(artworks, column_name, occurrences_threshold)[2]\n",
    "        dummies_for_segment = prepare_dataframe_dummies(artworks, column_name, segments_dfs)\n",
    "        # concat dummies_for_segment to dummies_for_all_segments\n",
    "        dummies_for_all_segments = pd.concat([dummies_for_all_segments, dummies_for_segment], axis=1)\n",
    "\n",
    "\n",
    "    dummies_for_all_segments.drop(columns=columns_names, inplace=True)\n",
    "\n",
    "    # add price, size, country\n",
    "    columns_to_add = ['Size (in²)', 'Price (US$)']\n",
    "    for column_name in columns_to_add:\n",
    "        dummies_for_all_segments[column_name] = artworks[column_name]\n",
    "\n",
    "    return dummies_for_all_segments\n",
    "\n",
    "\n",
    "def analyse_by_segments(artworks, column_name, sort_by, min_frequency):\n",
    "    artworks_by_segment = analyse_by_column(artworks, column_name, min_frequency)\n",
    "    # Get mean price per segment as 'Mean Price' column\n",
    "    mean_price_by_segment = artworks_by_segment.groupby(column_name).agg({'Price (US$)': 'mean'})\n",
    "    median_price_by_segment = artworks_by_segment.groupby(column_name).agg({'Price (US$)': 'median'})\n",
    "    mean_price_per_in_by_segment = artworks_by_segment.groupby(column_name).agg({'Price (US$/in²)': 'mean'})\n",
    "    median_price_per_in_by_segment = artworks_by_segment.groupby(column_name).agg({'Price (US$/in²)': 'median'})\n",
    "    col_analysis_by_segment = pd.concat([mean_price_by_segment, median_price_by_segment, mean_price_per_in_by_segment, median_price_per_in_by_segment, artworks[column_name].value_counts()], axis=1)\n",
    "    col_analysis_by_segment.columns = ['Mean Price (US$)', 'Median Price (US$)','Mean Price (US$/in²)', 'Median Price (US$/in²)', 'Count (#)']\n",
    "    # filter out segments with less than [min_frequency] artworks\n",
    "    col_analysis_by_segment = col_analysis_by_segment[col_analysis_by_segment['Count (#)'] > min_frequency]\n",
    "\n",
    "    col_analysis_by_segment.sort_values(by=sort_by, ascending=False, inplace=True)\n",
    "    col_analysis_by_segment = col_analysis_by_segment.dropna()\n",
    "\n",
    "    return col_analysis_by_segment\n",
    "\n",
    "\n",
    "def get_descriptive_stats_of_columns(dataframe, columns_names, segment=\"\"):\n",
    "\n",
    "    descriptive_stats_df = pd.DataFrame()\n",
    "\n",
    "    for column_name in columns_names:\n",
    "        column = dataframe[column_name]\n",
    "        \n",
    "        descriptive_stats_df.loc['max', column_name] = column.max()\n",
    "        descriptive_stats_df.loc['min', column_name] = column.min()\n",
    "        descriptive_stats_df.loc['mean', column_name] = column.mean()\n",
    "        descriptive_stats_df.loc['median', column_name] = column.median()\n",
    "        descriptive_stats_df.loc['stdev', column_name] = column.std()\n",
    "        descriptive_stats_df.loc['var', column_name] = column.var()\n",
    "\n",
    "        modified_column_name = column_name.replace(' ', '_').replace('(', '').replace(')', '').replace('/', '_')\n",
    "        descriptive_stats_df.to_csv(f'data_files/descriptive_analysis_{modified_column_name}{segment}.csv', index=True)\n",
    "        plot_histogram(column, f'Artworks by {column_name}', column_name, 'Frequency')\n",
    "\n",
    "    return descriptive_stats_df\n",
    "\n",
    "\n",
    "\n",
    "def case_study(dataframe, indices_lists, segments_names, case_study_name=\"\"):\n",
    "    for indices_list, segment in zip(indices_lists, segments_names):\n",
    "        size_dummies = dataframe.loc[indices_list]\n",
    "        size_dummies['Predicted Price (US$)'] = size_dummies['Predicted Price (US$)'].astype(float)\n",
    "\n",
    "        print(f'\\n\\n{segment} size artworks')\n",
    "\n",
    "        descriptive_df_segments = get_descriptive_stats_of_columns(size_dummies, ['Residual Price (%)', 'Residual Price (US$)'], f'_{case_study_name}_{segment}')\n",
    "        print(descriptive_df_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODELS\n",
    "\n",
    "\n",
    "def get_stats(x, y):\n",
    "    stats = {}\n",
    "    for variable in [x, y]:\n",
    "        max_variable = round(max(variable), 2)\n",
    "        min_variable = round(min(variable), 2)\n",
    "        mean_variable = round(variable.mean(), 2)\n",
    "        median_variable = round(variable.median(), 2)\n",
    "        stats_variable = {'Max': max_variable, 'Min': min_variable, 'Mean': mean_variable, 'Median': median_variable}\n",
    "        # add stats_variable to stats\n",
    "        stats[variable] = stats_variable\n",
    "    return stats\n",
    "\n",
    "\n",
    "## GET MODELS\n",
    "\n",
    "def get_decision_tree(X_train, y_train):\n",
    "    decision_tree = DecisionTreeRegressor(random_state=42)\n",
    "    decision_tree.fit(X_train, y_train)\n",
    "    return decision_tree\n",
    "\n",
    "def get_linear_regression(X_train, y_train):\n",
    "    linear_regression = LinearRegression()\n",
    "    linear_regression.fit(X_train, y_train)\n",
    "    return linear_regression\n",
    "\n",
    "def get_random_forest(X_train, y_train):\n",
    "    rf_model = RandomForestRegressor(random_state=1)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    return rf_model\n",
    "\n",
    "def get_gradient_boosting(X_train, y_train):\n",
    "    # get gradient boosting model\n",
    "    gb_model = GradientBoostingRegressor(random_state=1)\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    return gb_model\n",
    "\n",
    "# gives a dataframe taking models as columns and score as rows\n",
    "def get_all_models(x_train, x_test, y_train, y_test):\n",
    "    \n",
    "    models = {'Linear regression': get_linear_regression(x_train, y_train),\n",
    "              'Decision tree': get_decision_tree(x_train, y_train),\n",
    "              'Random forest': get_random_forest(x_train, y_train),\n",
    "              'Gradient boosting': get_gradient_boosting(x_train, y_train)}\n",
    "    \n",
    "    models_df = pd.DataFrame(columns=models.keys())\n",
    "    for model_name, model in models.items():\n",
    "        y_pred = model.predict(x_test)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        n = len(y_test)\n",
    "        p = x_test.shape[1]\n",
    "        adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "        mean_error = mean_absolute_error(y_test, y_pred)\n",
    "        median_error = median_absolute_error(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        pearson = np.corrcoef(y_test.to_numpy(), y_pred)[0, 1]\n",
    "\n",
    "        models_df.loc['R² Score', model_name] = r2\n",
    "        models_df.loc['Adjusted R² Score', model_name] = adj_r2\n",
    "        models_df.loc['Pearson Correl Predicted-Actual', model_name] = pearson\n",
    "        models_df.loc['Mean Absolute Error', model_name] = mean_error\n",
    "        models_df.loc['Median Absolute Error', model_name] = median_error\n",
    "        models_df.loc['Mean Squared Error', model_name] = mse\n",
    "\n",
    "        # Round values\n",
    "        models_df = models_df.round(2)\n",
    "\n",
    "    return models_df, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHARTS\n",
    "\n",
    "dark_green_color_code = '#195921'\n",
    "gold_color_code = '#FFD700'\n",
    "black_color_code = '#000000'\n",
    "salmon_color_code = '#FA8072'\n",
    "\n",
    "def plot_segment_chart(dataframe, segments_column_name, bar_column_name, line_columns_names, title):\n",
    "    bar_color_code = dark_green_color_code\n",
    "    line_colors_codes = [gold_color_code, black_color_code]\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.bar(dataframe.index, dataframe[bar_column_name], color=bar_color_code)\n",
    "    for line_column_name in line_columns_names:\n",
    "        ax2.plot(dataframe.index, dataframe[line_column_name], label=line_column_name, color=line_colors_codes.pop(0))\n",
    "    ax2.legend()\n",
    "    ax1.set_xticklabels(dataframe.index, rotation=90)\n",
    "    ax1.set_ylabel(bar_column_name)\n",
    "    ax2.set_ylabel(str(line_columns_names))\n",
    "    ax1.set_xlabel(segments_column_name)\n",
    "    ax1.set_title(title)\n",
    "    # beautify and enhance readability of the chart\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "\n",
    "    ax1.tick_params(axis='x', which='major', labelsize=8)\n",
    "    ax1.tick_params(axis='y', which='major', labelsize=8)\n",
    "    ax2.tick_params(axis='y', which='major', labelsize=8)\n",
    "    plt.savefig('./charts/' + title.lower().replace(' ', '_').replace('(', '').replace(')', '').replace('/', '_') + '.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_scatter(dataframe, x_column_name, y_column_name, title, log=False):\n",
    "    if log:\n",
    "        x = np.log(dataframe[x_column_name].astype(float))\n",
    "        y = np.log(dataframe[y_column_name].astype(float))\n",
    "    else:\n",
    "        x = dataframe[x_column_name].astype(float)\n",
    "        y = dataframe[y_column_name].astype(float)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_column_name)\n",
    "    plt.ylabel(y_column_name)\n",
    "    plt.tick_params(axis='x', which='major', labelsize=8)\n",
    "    plt.tick_params(axis='y', which='major', labelsize=8)\n",
    "    \n",
    "    plt.scatter(x=x, y=y, c=dark_green_color_code)\n",
    "\n",
    "    z = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(x,p(x),c=salmon_color_code)\n",
    "    plt.savefig('./charts/' + title.lower().replace(' ', '_').replace('(', '').replace(')', '').replace('/', '_') + '.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_histogram(column, title, x_label, y_label, bins=50):\n",
    "    column.hist(bins=bins, color=dark_green_color_code)\n",
    "    # line of the mean\n",
    "    plt.axvline(column.mean(), color='gold', linestyle='dashed', linewidth=1)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.savefig('./charts/' + title.lower().replace(' ', '_').replace('(', '').replace(')', '').replace('/', '_') + '.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INITIALIZE DATAFRAME\n",
    "\n",
    "artworks = get_artworks_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DESCRIPTIVE STATISTICS OF COLUMNS PRICE, PRICE PER INCH AND SIZE\n",
    "\n",
    "\n",
    "columns_names = ['Price (US$)', 'Price (US$/in²)', 'Size (in²)']\n",
    "\n",
    "descriptive_stats_df = get_descriptive_stats_of_columns(artworks, columns_names)\n",
    "\n",
    "## SCATTER PLOT PRICE VS SIZE\n",
    "plot_scatter(artworks, 'Size (in²)', 'Price (US$)', 'Size vs Price', True)\n",
    "\n",
    "descriptive_stats_df.to_csv('data_files/descriptive_stats.csv', index=True)\n",
    "descriptive_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DESCRIPTIVE STATISTICS OF COUNTRY COLUMN\n",
    "\n",
    "column_name1 = 'Country'\n",
    "sort_by1 = 'Count (#)'\n",
    "min_frequency1 = artworks.shape[0] * 0.01\n",
    "\n",
    "col_analysis_by_segment = analyse_by_segments(artworks, column_name1, sort_by1, min_frequency1)\n",
    "plot_segment_chart(col_analysis_by_segment, column_name1, 'Count (#)', [], 'Artworks by ' + column_name1)\n",
    "col_analysis_by_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DESCRIPTIVE STATISTICS OF CATEGORIES (ARTIST, STYLES, SUBJECTS, MEDIUMS, MATERIALS)\n",
    "\n",
    "categories_columns_list = ['Artist', 'Styles', 'Subjects', 'Mediums', 'Materials']\n",
    "\n",
    "sort_by = 'Median Price (US$/in²)'\n",
    "\n",
    "categories_dfs = {}\n",
    "categories_models_metrics_df = {}\n",
    "categories_models = {}\n",
    "\n",
    "for column_name in categories_columns_list:\n",
    "\n",
    "    if column_name == 'Artist':\n",
    "        occurrences_threshold = 5\n",
    "    else:\n",
    "        occurrences_threshold = artworks.shape[0]/100\n",
    "\n",
    "    column = artworks[column_name]\n",
    "    filtered_artworks_data, all_segments_from_category_df, segments_dfs = segment_and_clean_data(artworks, column_name, occurrences_threshold)\n",
    "\n",
    "    all_segments_from_category_df.drop('Variance (US$/in²)Count (#)', axis=1, inplace=True)\n",
    "    all_segments_from_category_df.sort_values(by=sort_by, ascending=False, inplace=True)\n",
    "\n",
    "    plot_segment_chart(all_segments_from_category_df, column_name, 'Count (#)', ['Mean Price (US$/in²)', 'Median Price (US$/in²)'], 'Artworks by ' + column_name)\n",
    "    categories_dfs[column_name] = all_segments_from_category_df\n",
    "\n",
    "    # save all_segments_from_category_df as csv\n",
    "    all_segments_from_category_df.to_csv(f'data_files/descriptive_analysis_{column_name}{quantile}.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## APPLYING MODELS TO SIZE AND CATEGORIES' DUMMIES\n",
    "\n",
    "artworks_for_models = artworks[['Artist', 'Styles', 'Subjects', 'Mediums', 'Materials', 'Size (in²)', 'Price (US$)', 'Price (US$/in²)']]\n",
    "\n",
    "train_data, test_data = train_test_split(artworks_for_models, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "train_dummies = get_dummies_for_all_segments(train_data, categories_columns_list)\n",
    "test_dummies = get_dummies_for_all_segments(test_data, categories_columns_list)\n",
    "\n",
    "# Deal with differences in columns between train and test\n",
    "missing_cols = set(train_dummies.columns) - set(test_dummies.columns)\n",
    "for missing_col in missing_cols:\n",
    "    test_dummies[missing_col] = False\n",
    "# Ensure the order of column in the test set is in the same order than in train set\n",
    "test_dummies = test_dummies[train_dummies.columns]\n",
    "\n",
    "# eliminate duplicate columns\n",
    "train_dummies = train_dummies.loc[:,~train_dummies.columns.duplicated()]\n",
    "test_dummies = test_dummies.loc[:,~test_dummies.columns.duplicated()]\n",
    "\n",
    "x_train = train_dummies.drop(columns=['Price (US$)'])\n",
    "y_train = train_dummies['Price (US$)']\n",
    "\n",
    "x_test = test_dummies.drop(columns=['Price (US$)'])\n",
    "y_test = test_data['Price (US$)']\n",
    "\n",
    "\n",
    "models_metrics_df, models = get_all_models(x_train, x_test, y_train, y_test)\n",
    "# # save models_metrics_df as csv\n",
    "models_metrics_df.to_csv(f'./data_files/models_metrics_df{quantile}.csv', index=True)\n",
    "\n",
    "models_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FEATURE IMPORTANCES\n",
    "\n",
    "# Select model with lowest MAE\n",
    "model_name = models_metrics_df.sort_values('Mean Absolute Error', axis=1, ascending=True).columns[0]\n",
    "model = models[model_name]\n",
    "\n",
    "feature_importances_df = pd.DataFrame(model.feature_importances_,\n",
    "                                      index = x_test.columns,\n",
    "                                        columns=['Feature Importance']).sort_values('Feature Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CATEGORY-CLUSTERED FEATURE IMPORTANCES\n",
    "\n",
    "# {Category: Segment} dictionay\n",
    "category_segments_dict = {}\n",
    "for column_name in categories_columns_list:\n",
    "    categories = categories_dfs[column_name].index.tolist()\n",
    "    category_segments_dict[column_name] = categories\n",
    "\n",
    "# Invert dictionary: {Segment: Category}\n",
    "segment_categories_dict = {}\n",
    "segment_categories_dict['Size (in²)'] = 'Size (in²)'\n",
    "for category, segments in category_segments_dict.items():\n",
    "    for segment in segments:\n",
    "        segment_categories_dict[segment] = category\n",
    "\n",
    "feature_importances_df['Category'] = feature_importances_df.index.map(segment_categories_dict)\n",
    "\n",
    "feature_importances_df\n",
    "\n",
    "clustered_feature_importances_df = feature_importances_df.groupby('Category').sum().sort_values('Feature Importance', ascending=False).head(10)\n",
    "\n",
    "# save to csv\n",
    "feature_importances_df.to_csv(f'data_files/model_feature_importances{quantile}.csv')\n",
    "clustered_feature_importances_df.to_csv(f'data_files/model_clustered_feature_importances{quantile}.csv')\n",
    "clustered_feature_importances_df\n",
    "\n",
    "# bar plot\n",
    "plot_segment_chart(clustered_feature_importances_df, 'Category', 'Feature Importance', [], 'Feature Importance by Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INCLUDE PREDICTIONS AND RESIDUALS IN TEST DATA\n",
    "\n",
    "test_dummies\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "test_dummies['Predicted Price (US$)'] = y_pred\n",
    "test_dummies['Residual Price (US$)'] = test_dummies['Predicted Price (US$)'] - test_dummies['Price (US$)']\n",
    "test_dummies['Residual Price (%)'] = test_dummies['Residual Price (US$)'] / test_dummies['Price (US$)']\n",
    "test_dummies['Price (US$/in²)'] = test_dummies['Price (US$)'] / test_dummies['Size (in²)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DISTRIBUTION OF RESIDUALS\n",
    "segment = \"\"\n",
    "descriptive_df_size_segments = get_descriptive_stats_of_columns(test_dummies, ['Residual Price (%)', 'Residual Price (US$)'], f'_{segment}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CASE STUDIES: LARGE RESIDUALS\n",
    "\n",
    "# get 0.75 quantile of residuals\n",
    "q75 = test_dummies['Residual Price (%)'].quantile(0.75)\n",
    "q25 = test_dummies['Residual Price (%)'].quantile(0.25)\n",
    "# get the artworks with index in large_residual_indices\n",
    "large_residual_indices = test_dummies[test_dummies['Residual Price (%)'] > q75].index\n",
    "small_residual_indices = test_dummies[test_dummies['Residual Price (%)'] < q25].index\n",
    "medium_residual_indices = test_dummies[(test_dummies['Residual Price (%)'] > q25) & (test_dummies['Residual Price (%)'] < q75)].index\n",
    "\n",
    "residual_indices_lists = [large_residual_indices, medium_residual_indices, small_residual_indices]\n",
    "resisual_names = ['big', 'medium', 'small']\n",
    "\n",
    "case_study(test_dummies, residual_indices_lists, resisual_names, \"residuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residuals (%) x size\n",
    "residuals_analysis_columns_names = ['Size (in²)', 'Price (US$)']\n",
    "\n",
    "for column_name in residuals_analysis_columns_names:\n",
    "    plot_scatter(test_dummies, column_name, 'Residual Price (US$)', f'Residuals (US$) x {column_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CASE STUDIES: SIZE SEGMENTS\n",
    "\n",
    "# get 0.75 quantile of size\n",
    "q75_size = np.percentile(test_dummies['Size (in²)'], 75)\n",
    "q25 = np.percentile(test_dummies['Size (in²)'], 25)\n",
    "\n",
    "big_size_indices = test_dummies[test_dummies['Size (in²)'] >= q75_size].index\n",
    "medium_size_indices = test_dummies[(test_dummies['Size (in²)'] > q25) & (test_dummies['Size (in²)'] < q75_size)].index\n",
    "small_size_indices = test_dummies[test_dummies['Size (in²)'] <= q25].index\n",
    "\n",
    "size_indices = [big_size_indices, medium_size_indices, small_size_indices]\n",
    "size_names = ['Big', 'Medium', 'Small']\n",
    "\n",
    "case_study(test_dummies, size_indices, size_names, \"size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CASE STUDIES: SELECTED ARTISTS\n",
    "\n",
    "# Select model\n",
    "model = models[model_name]\n",
    "\n",
    "# Select artists\n",
    "selected_artists = artworks['Artist'].value_counts().head(9).index\n",
    "\n",
    "artists_names = selected_artists\n",
    "print(artists_names)\n",
    "\n",
    "artists_indices = []\n",
    "for artist_name in artists_names:\n",
    "    artist_name = artist_name[0]\n",
    "    artist_indices = test_dummies[test_dummies[artist_name] == True].index\n",
    "    artists_indices.append(artist_indices)\n",
    "\n",
    "    artist_dummies = test_dummies.loc[artist_indices]\n",
    "    artist_dummies['Title'] = artworks.loc[artist_indices]['Title']\n",
    "    \n",
    "    # Assuming artist_arworks_dummies is your DataFrame\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    # Plotting Price Prediction and Price against Artwork Titles\n",
    "    ax.plot(artist_dummies['Title'], artist_dummies['Predicted Price (US$)'], label='Predicted Price (US$)')\n",
    "    ax.plot(artist_dummies['Title'], artist_dummies['Price (US$)'], label='Price (US$)')\n",
    "    # Rotating x-axis labels for better readability\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    ax.set_xlabel('Artworks')\n",
    "    ax.set_ylabel('Price (US$)')\n",
    "    ax.set_title('Price Prediction vs Price for {}'.format(artist_name))\n",
    "    ax.legend()\n",
    "    plt.tight_layout()  # To ensure labels and titles fit properly\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "case_study(test_dummies, artists_indices, artists_names, \"artists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIND BETA OF FEATURES \n",
    "\n",
    "x_beta_dummies = test_dummies.drop(columns=['Residual Price (%)', 'Residual Price (US$)', 'Predicted Price (US$)', 'Price (US$/in²)', 'Price (US$)'])\n",
    "y_beta_dummies = test_dummies['Predicted Price (US$)']\n",
    "\n",
    "linear_regression = get_linear_regression(x_beta_dummies, y_beta_dummies)\n",
    "\n",
    "beta_df = pd.DataFrame(linear_regression.coef_, index=x_beta_dummies.columns, columns=['Beta']).sort_values('Beta', ascending=False)\n",
    "\n",
    "# merge beta_df with feature_importances_df\n",
    "beta_df = pd.merge(beta_df, feature_importances_df, left_index=True, right_index=True).drop(columns=['Feature Importance'])\n",
    "\n",
    "beta_df.head(20).to_csv(f'data_files/beta_df_top_beta.csv')\n",
    "beta_df.tail(20).to_csv(f'data_files/beta_df_bottom_beta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET ARTISTS COUNTRIES\n",
    "\n",
    "artists = feature_importances_df[feature_importances_df['Category'] == 'Artist'].index.tolist()\n",
    "\n",
    "artists_countries = pd.DataFrame(columns=['Country'], index=artists)\n",
    "for artist in artists:\n",
    "    artist_country = artworks[artworks['Artist'].apply(lambda x: artist in x)].Country.unique().tolist()[0]\n",
    "    artists_countries.loc[artist, 'Country'] = artist_country\n",
    "\n",
    "artists_df = feature_importances_df.merge(beta_df, left_index=True, right_index=True)\n",
    "artists_df = artists_df[artists_df['Category'] == 'Artist']\n",
    "artists_df = artists_df.merge(artists_countries, left_index=True, right_index=True)\n",
    "artists_df.sort_values('Beta', ascending=False, inplace=True)\n",
    "artists_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COUNTRIES OF TOP ARTISTS\n",
    "\n",
    "top_artists = artists_df.sort_values('Beta', ascending=False).head(100)\n",
    "top_artists_countries = list(set(top_artists['Country'].to_list()))\n",
    "\n",
    "bottom_artists = artists_df.sort_values('Beta', ascending=True).head(100)\n",
    "top_artists_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COUNTRIES DF\n",
    "\n",
    "countries_df = artists_df.groupby('Country').agg({'Beta': 'mean', 'Feature Importance': 'sum'}).sort_values('Beta', ascending=False)\n",
    "countries_df\n",
    "# top_artists_countries = artists_df.sort_values('Beta', ascending=False).head(100).Country.tolist()\n",
    "# top_artists_countries = artists_df.sort_values('Beta', ascending=False).tail(100).Country.tolist()\n",
    "\n",
    "# print('Top artists:', top_artists)\n",
    "# print('Bottom artists:', bottom_artists)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
